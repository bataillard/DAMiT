{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode choice prediction for non-car owning households in the USA\n",
    "**Decision-aid methodologies in transportation, EPFL Spring 2021**\n",
    "\n",
    "Florent Zolliker, Gaelle Abi Younes, Luca Bataillard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data pre-processing\n",
    "\n",
    "In this step, we will process and adjust the dataset in order to facilitate our model training. We begin by importing the datasets and relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate = pd.read_csv(\"nhts_train_validate_revised.csv\", index_col=\"TRIPID\")\n",
    "test = pd.read_csv(\"nhts_test_revised.csv\", index_col=\"TRIPID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to consider the features and their format in order to select the appropriate ones. \n",
    "\n",
    "We first notice a group of context columns, that are not relevant to model training:\n",
    "* `TRIPID`: trip identifier, indexes the dataset but otherwise not relevant for training or grouped sampling.\n",
    "* `HOUSEID`: household identifier, this is the topmost hiearchical group in the survey. This column should be used to perform grouped samping during cross-validation\n",
    "* `PERSONID`: person identifier, this is another hiearchical group, but since `HOUSEID` is higher in the hiearchy, using it is not necessary.\n",
    "* `TDTRPNUM`: trip numbering per person in the survey.\n",
    "\n",
    "The target label is `TRAVELMODE`. This label is categorical, so we will encode the values using a simple numeric encoding. \n",
    "\n",
    "We also notice a `TRPTRANS` column that is very highly correllated with `TRAVELMODE` but does not feature in the `nhts_dictionary.csv` file provided with the project. After inspecting the NHTS documentation online, we suspect that the travel mode column was most likely generated from this column. We will thus discard the `TRPTRANS`column. Furthermore, negative responses in the `TRPTRANS` column resulted in `NaN` values in `TRAVELMODE`, that need to be filtered out.\n",
    "\n",
    "Let us analyse the remaining columns. Missing values indicates that some values for that column are invalid in the dataset. In the case of categorical, this does not pose a big problem, since \n",
    "\n",
    "| Column name | Missing values | Categorical data | One-hot encoding | Scaling | Use feature? | Description | Comments |\n",
    "| ---         | ---            | ---              | ---              | ---     | ---          | ---         | ---      |\n",
    "| `STRTTIME`  |  -  |  -  |  -  | yes | yes | start time of trip | |\n",
    "| `TRPMILES`  | yes |  -  |  -  | yes | yes | length of trip in miles ||\n",
    "| `LOOP_TRIP` |  -  | yes | yes |  -  | yes | same origin and destination | binary variable, applying `mod 2` to this column can replace one-hot encoding |\n",
    "| `TRIPPURP`  |  -  | yes | yes |  -  | yes | trip purpose ||\n",
    "| `TRAVDAY`   |  -  | yes |  -  | yes | yes | weekday of travel ||\n",
    "| `HOMEOWN`   | yes | yes | yes |  -  | yes | home ownership ||\n",
    "| `HHSIZE`    |  -  |  -  |  -  | yes | yes | size of household ||\n",
    "| `HHFAMINC`  | yes | yes |  -  | yes | yes | household income ||\n",
    "| `HHSTATE`   |  -  | yes | yes |  -  |  ?  | household state of residency | could use either `HHSTATE` or `CENSUS_D` |\n",
    "| `WRKCOUNT`  |  -  |  -  |  -  | yes | yes | number of workers in household ||\n",
    "| `LIF_CYC`   |  -  |  -  |  -  | yes | yes | life cycle classification ||\n",
    "| `URBAN`     |  -  | yes |  ?  |  ?  | yes | classification of urban area ||\n",
    "| `URBANSIZE` |  -  | yes |  ?  |  ?  |  ?  | population size of urban area | redundant with `URBAN`, needs reordering or one-hot encoding |\n",
    "| `CENSUS_D`  |  -  | yes | yes |  -  |  -  | census division (region) of household | could be redundant with `HHSTATE` |\n",
    "| `HH_RACE`   | yes | yes | yes |  -  |  yes  | race of household | |\n",
    "| `EDUC`      | yes | yes |  -  | yes |  yes  | educational attainment of household | |\n",
    "| `WORKER`    | yes | yes | yes |  -  |  yes  | worker status | |\n",
    "| `WHYTRP90`  | yes | yes | yes |  -  |  yes  | trip purpose with 1990 NPTS design | possible duplicate of `TRIPPURP`|\n",
    "| `R_AGE_IMP` |  -  |  -  |  -  | yes |  yes  | age | |\n",
    "| `R_SEX_IMP` |  -  | yes | yes |  -  |  yes  | gender | |\n",
    "| `OBHUR`     | yes | yes |  -  |  -  |  yes  | urban/rural indicator at origin | |\n",
    "| `DBHUR`     | yes | yes |  -  |  -  |  yes  | urban/rural indicator at destination | |\n",
    "| `OBPPOPDN`  | yes | yes |  -  |  -  |   -   | population density at origin | already covered by `OBHUR` |\n",
    "| `DBPPOPDN`  | yes | yes |  -  |  -  |   -   | population density at destination | already covered by `DBHUR` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column and topmost hiearchical sampling column\n",
    "target = \"TRAVELMODE\"\n",
    "group = \"HOUSEID\"\n",
    "\n",
    "# Context columns (not used here) \n",
    "context_cols = [\n",
    "    \"TRIPID\", \"HOUSEID\", \"PERSONID\", \"TDTRPNUM\"\n",
    "]\n",
    "\n",
    "# Columns used as features that do not need a specific encoding \n",
    "no_changes = [\n",
    "    \"STRTTIME\", \"TRPMILES\", \"TRAVDAY\", \"HHSIZE\", \"HHFAMINC\", \"WRKCOUNT\", \n",
    "    \"LIF_CYC\", \"URBAN\", \"EDUC\", \"R_AGE_IMP\", \"DBPPOPDN\", \"OBPPOPDN\"\n",
    "]\n",
    "\n",
    "# Categorical columns that need one-hot encoding and the values they can take\n",
    "one_hot_encodings = {\n",
    "    \"TRIPPURP\": [\"HBO\", \"HBSHOP\", \"HBSOCREC\", \"HBW\", \"NHB\", -9],\n",
    "    \"HOMEOWN\": [1, 2, 97, -8, -7],\n",
    "    \"HHSTATE\": [\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \n",
    "                   \"KS\",\"KY\", \"LA\", \"MA\", \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\",\"NM\",\n",
    "                    \"NV\",\"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \n",
    "                    \"WV\", \"WY\"],\n",
    "    \"CENSUS_D\": [c for c in range(1, 10)],\n",
    "    \"HH_RACE\": [1, 2, 3, 4, 5, 6, 97, -7, -8], \n",
    "    \"WORKER\": [1, 2, -9, -1],\n",
    "    \"WHYTRP90\": [1, 2, 3, 4, 5, 6, 8, 10, 11, 99],\n",
    "    \"R_SEX_IMP\": [1, 2],\n",
    "}\n",
    "\n",
    "# Columns that need some in-place transformation (such as numerical label encoding) \n",
    "hur_encoding = lambda x: [\"-9\", \"R\", \"T\", \"C\", \"S\", \"U\"].index(x)\n",
    "label_encodings = {\n",
    "    \"LOOP_TRIP\": lambda x: x % 2,\n",
    "    \"URBANSIZE\": lambda x: (x % 6) + 1,\n",
    "    \"OBHUR\": hur_encoding,\n",
    "    \"DBHUR\": hur_encoding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, col_cat_encodings):\n",
    "    \"\"\"\n",
    "    Takes a NHTS pandas dataset and a dictionary mapping categorical columns to their\n",
    "    possible values. \n",
    "    Returns a new dataset with all categorical column one-hot encoding. A column of zeroes\n",
    "    is created for each value that appears in the list of possible values but not in actual data\n",
    "    \n",
    "    TODO: Check out https://www.algosome.com/articles/dummy-variable-trap-regression.html\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = set([col + \":\" + str(cat) for col, cats in col_cat_encodings.items() for cat in cats])\n",
    "    original_columns = set(df.columns)\n",
    "    new_columns = (original_columns | labels) - col_cat_encodings.keys()\n",
    "    \n",
    "    df_one_hot = pd.get_dummies(df, columns=col_cat_encodings.keys(), prefix_sep=\":\")\n",
    "    df_full = df_one_hot.reindex(columns=new_columns, fill_value=0)\n",
    "    \n",
    "    return df_full\n",
    "\n",
    "def process_dataset(df):\n",
    "    \"\"\"\n",
    "    Takes a pandas dataset in the NHTS survey format, keeps only columns in `no_changes`,\n",
    "    `one_hot_encodings` and `label_encodings`. One-hot encodes features in `one_hot_encodings`\n",
    "    and generates numeric labels for columns in `label_encodings`\n",
    "    Returns a new dataframe containing the transformed features.\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = [*no_changes, *one_hot_encodings.keys(), *label_encodings.keys()]\n",
    "    features = df[columns]\n",
    "    \n",
    "    X = one_hot_encode(features, one_hot_encodings)\n",
    "    for column, encoding in label_encodings.items():\n",
    "        X[column] = X[column].map(encoding)\n",
    "        \n",
    "    return X\n",
    "\n",
    "def process_target(y):\n",
    "    return y.replace(target_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate_no_nans = train_validate.dropna(axis=\"index\")\n",
    "test_no_nans = test.dropna(axis=\"index\")\n",
    "\n",
    "target_encoding = {mode: index for index, mode in \n",
    "                   enumerate(sorted(train_validate_no_nans[target].unique().tolist()))}\n",
    "\n",
    "X = process_dataset(train_validate_no_nans)\n",
    "y = process_target(train_validate_no_nans[target])\n",
    "groups = train_validate_no_nans[group]\n",
    "\n",
    "X_test = process_dataset(test_no_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional step to our algorithm is to apply scaling to our values, which is needed with distance-based models such as logistic classifiers and k-nearest neighbours.\n",
    "\n",
    "For indicator variables, no scaling is necessary. For other values, we can apply min-max or standard scaling. Standard scaling should be used when data is supposed to be normally distributed. Given that there are very few features where that assumption can be confidently made, we will apply min-max scaling to all scalable columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_scaled, X_test_scaled = X.copy(), X_test.copy()\n",
    "\n",
    "scaled_columns = no_changes + list(label_encodings.keys())\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_scaled[scaled_columns])\n",
    "\n",
    "\n",
    "\n",
    "X_scaled[scaled_columns] = scaler.transform(X_scaled[scaled_columns])\n",
    "X_test_scaled[scaled_columns] = scaler.transform(X_test_scaled[scaled_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Hyperparameter search and model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project makes use of several heavy objects that take a significant amount of time to create. To reduce recomputation time when running the notebook again, we define a memoization utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import os, joblib\n",
    "\n",
    "def memoize(filename, func, *fargs, **fkwargs):\n",
    "    \"\"\"\n",
    "    On first call, calls `func` with arguments `fargs` and `fkwargs`. Saves returned object to memo/`filename`.joblib.\n",
    "    Subsequent calls retreive object from file. Returns saved object.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_filename = \"memo/\" + filename + \".joblib\"\n",
    "    \n",
    "    @wraps(func)\n",
    "    def wrapper():\n",
    "        if not os.path.isfile(full_filename):\n",
    "            obj = func(*fargs, **fkwargs)\n",
    "            joblib.dump(obj, full_filename)\n",
    "            return obj\n",
    "        else:\n",
    "            return joblib.load(full_filename)\n",
    "    \n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a generalised way of performing hyperparameter tuning. This will be applied to all models tried out throughout this notebook. We create a decorated memoized version of the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GroupKFold\n",
    "\n",
    "def hyperparam_search(classifier, params, X=X, y=y, groups=groups, verbose=100, fit_params=None):\n",
    "    \"\"\"\n",
    "    Takes a classifier, a dictionary of hyperparameter distributions and an (X,y,groups) dataset\n",
    "    and returns a SearchCV object. The returned object is also saved to `filename` if it not none.\n",
    "    Defaults to using non-scaled (X,y,groups).\n",
    "    \"\"\"\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    search = RandomizedSearchCV(classifier, params,\n",
    "                                scoring=\"neg_log_loss\",\n",
    "                                cv=gkf, \n",
    "                                n_jobs=-1, \n",
    "                                verbose=verbose, \n",
    "                                random_state=SEED)\n",
    "\n",
    "    search.fit(X, y.squeeze(), groups=groups, **fit_params)\n",
    "    \n",
    "    return search\n",
    "\n",
    "@wraps(hyperparam_search)\n",
    "def m_hyperparam_search(filename, *fargs, **fkwargs):\n",
    "    return memoize(filename, hyperparam_search, *fargs, **fkwargs)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "The first model we will consider is XGBoost. This model does not require feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   58.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  2.6min\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "xbg_params = {\n",
    "    \"n_estimators\": randint(20, 500),\n",
    "    \"max_depth\": randint(1, 5),\n",
    "    \"learning_rate\": uniform(0.5,1),\n",
    "    \"reg_lambda\": uniform(0.1, 2), \n",
    "}\n",
    "\n",
    "xgb_fit_params = {\n",
    "    'early_stopping_rounds': 10,\n",
    "    'eval_set': [[X,y]]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(objective=\"multi:softprob\",\n",
    "                    num_class=len(y.squeeze().unique()),\n",
    "                    random_state=SEED,\n",
    "                    verbose=0,\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='mlogloss')\n",
    "\n",
    "\n",
    "xgbsearch = m_hyperparam_search(\"xgb_search\", xgb, xbg_params, fit_params=xgb_fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = xgbsearch.best_estimator_\n",
    "best_xbg = memoize(\"best_xbg\", lambda: xgbsearch.best_estimator_.fit(X, y))()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_xbg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pandas = pd.DataFrame(y_pred, columns=target_encoding.keys(), index=X_test.index)\n",
    "y_pred_pandas.to_csv(\"best_xgb_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(xgbsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
